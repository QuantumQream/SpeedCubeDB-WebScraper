import requests as request
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
from random import randint
from time import sleep

#Here we list a few useful functions that extract different pieces of information from each url
#(based on the current format of speedcubedb.com as of May-2022)

#Simple function that gives the name of the solver by looking at the class 'recon-solver'
def find_name(soup):
    return soup.find_all('div', class_='recon-solver')[0].string


#We first find all the span tags with class 'notation comment' and grab the text (deleting an initial space ' ' at the start of each one)
#We also perform a check on the first element in order to make sure we don't count 'inspection' as a step
def find_steps(soup):
    array = soup.find_all('span',class_='notation comment')
    x = [step.text[1:] for step in array]

    if x[0]=='inspection':
        return x[1:]
    else: return x


#Simple function to grab the type of cross-solution the solver did
def find_cross_type(soup):
    return find_steps(soup)[0]


#Returns a boolean value based on the last element generated by the find_steps function
def find_zbll(soup):
    if find_steps(soup)[-1]=='ZBLL':
        return True
    else: return 

#Returns the length of the steps list
def find_num_steps(soup):
    return len(find_steps(soup))
    

#this function does much of the heavy lifting for us. 
#It's job is to grab all the available metrics from the table that is found on the bottom-right corner
#The original table on the website is in a 'groupby' structure, so we take extra care to organise the data in a one-metric-per-column fashion
#We add the name of the solver, the cross-type solution, the number of steps and the use of ZBLL
#Finally it returns a list of 40 columns, this corresponds to a single row of the DataFrame we will construct
def create_table_data(url):
    #Create the beautifulsoup object by parsing the url
    url=request.get(url)
    soup = BeautifulSoup(url.text,'html.parser')

    #Finding all the div tags with class info-header and then getting the text from the parent of that each div tag.
    #Create a list of the useful elements for each item in our scrape
    x = soup.find_all('div',class_='info-header')
    original_table = [item.parent.text.split('\n')[2:-1] for item in x]

    #Create lists for each metric accross each step (ordering them how we want to)
    metrics=[]
    for j in range(0,len(original_table)):
        for i in range(0,len(original_table)):
            metrics.append(original_table[i][j])

    #Clean the raw data by converting into floats and adding the solver's name
    clean_metrics=[find_name(soup)]
    for item in metrics:
        if item[-1]=='%':
            clean_metrics.append(float(item[:-1]))
        else:
            clean_metrics.append(float(item))
    
    #Adding a few useful metrics constucted from my predefined functions
    clean_metrics.append(find_cross_type(soup))
    clean_metrics.append(find_zbll(soup))
    clean_metrics.append(find_num_steps(soup))
    return clean_metrics

#The items in the list generated by the create_table_data function follows this specific order
cols=[
'name','total_time','f2l_time','ll_time','cross+1_time','ols_time','pll_time',
'total_time_percent','f2l_time_percent','ll_time_percent','cross+1_percent','ols_percent','pll_percent',
'total_stm','f2l_stm','ll_stm','cross+1_stm','ols_stm','pll_stm',
'total_stps','f2l_stps','ll_stps','cross+1_stps','ols_stps','pll_stps',
'total_etm','f2l_etm','ll_etm','cross+1_etm','ols_etm','pll_etm',
'total_etps','f2l_etps','ll_etps','cross+1_etps','ols_etps','pll_etps',
'cross_type','zbll','num_steps'
]

#We scan each page of the search reconstructions section filtered by cfop, sub7 and 3x3
#We look for all of the available solves in each page and grab their correspondent 'solve_id' (The number shown in the url: 'speedcubedb.com/r/solve_id')
#we  ignnore the last item since it returns '...'
def get_links(initial_url):
    list_of_url_pages = []
    for i in range(1,147):
        list_of_url_pages.append(initial_url+'&page={}'.format(i))

    solve_ids=[]
    for link in list_of_url_pages:
        url = request.get(link)
        soup = BeautifulSoup(url.text,'html.parser')
        tags = soup.find_all('a',class_='recontable-line', href=True)
        solve_ids = solve_ids + [item['href'] for item in tags]
        sleep(randint(2,5))
    return solve_ids[:-1]

#KNOWN BUG: For some reason it only grabs 1000 urls instead of the total ~7000
solve_ids = get_links('https://speedcubedb.com/r/index/?&search=&orderby=None&sortbyorder=asc&cfop=on&sub7=on&3x3=on')


#We feed all of our scraped urls into the create_table_data function and save our DataFrame as a csv file to our current directory
data=[]
for solve_id in solve_ids:
    data.append(create_table_data('https://speedcubedb.com' + solve_id))
    sleep(randint(2,5))


solves = pd.DataFrame(data=data,columns=cols)
cwd = os.getcwd()
path = cwd + "/solves"
solves.to_csv(path+'.csv')